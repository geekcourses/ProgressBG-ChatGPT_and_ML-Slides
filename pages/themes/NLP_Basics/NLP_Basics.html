<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>NLP_Basics</title>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <link rel="shortcut icon" href="/ProgressBG-ChatGPT_and_ML-Slides/images/favicons/favicon-32.png">
    <!-- css & themes include -->
    <link rel="stylesheet" href="/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="/ProgressBG-ChatGPT_and_ML-Slides/outfit/css/themes/light.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/css/print/pdf.css' : '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
    <!-- CUSTOM -->
    <base target="_blank">
</head>

<body>
    <div class="reveal default center" data-transition-speed="default" data-background-transition="default">
        <div class="top_links">
            <a class="home_link" href="/ProgressBG-ChatGPT_and_ML-Slides/pages/agenda/agenda.html#NLP_Basics"
                target="_top"><i class="fa fa-home"></i></a>
            <span class="help_link" href="#"><i class="fa fa-question"></i></span>
            <div class="help_text">
                <div class="note">Keyboard shortcuts:</div>
                <div><span>N/Space</span><span>Next Slide</span></div>
                <div><span>P</span><span>Previous Slide</span></div>
                <div><span>O</span><span>Slides Overview</span></div>
                <div><span>ctrl+left click</span><span>Zoom Element</span></div>
                <div class="print-howto"><br>If you want print version => add '<code>?print-pdf</code>' <br> at the end
                    of slides URL (remove '#' fragment) and then print. <br>
                    Like: https://ProgressBG-ChatGPT_and_ML-course.github.io/...CourseIntro.html?print-pdf </div>
            </div>
        </div>
        <div class="footer theme_switch">
            <a href="#"
                onclick="document.getElementById('theme').setAttribute('href','/ProgressBG-ChatGPT_and_ML-Slides/outfit/css/themes/dark.css'); return false;">Dark</a>
            <a href="#"
                onclick="document.getElementById('theme').setAttribute('href','/ProgressBG-ChatGPT_and_ML-Slides/outfit/css/themes/light.css'); return false;">Light</a>
            <a href="#"
                onclick="document.getElementById('theme').setAttribute('href','/ProgressBG-ChatGPT_and_ML-Slides/outfit/css/themes/projector.css'); return false;">Projector</a>
        </div>
        <div class="slides">
            <!--
########################################################
##################### SLIDES START #####################
########################################################
-->
            <section><h1>Natural Language Processing (NLP) Basics</h1>
            </section>
            <section data-transition="zoom">
                <!-- linkedin badge -->
                <!--<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>-->
                <section class="copyright" data-transition="zoom">
                    <div class="note">
                        <p>Created for</p>
                    </div>
                    <div class="company">
                        <a href="http://progressbg.net/програмиране-с-python-2/">
                            <img style="height:80%"
                                src="/ProgressBG-ChatGPT_and_ML-Slides/outfit/images/logos/ProgressBG_logo_529_127.png">
                        </a>
                    </div>
                    <div class="author">
                        <span class="note">Iva E. Popova, 2016-2025,</span>
                        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                                alt="Creative Commons License" style="border-width:0"
                                src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
                    </div>
                </section>
                <section class="copyright" data-transition="zoom" style="margin-top: -2em;">
                    <div class="company">
                        <div class="LI-profile-badge" data-version="v1" data-size="large" data-locale="en_US"
                            data-type="vertical" data-theme="dark" data-vanity="ivapopova"><a class="LI-simple-link"
                                href='https://bg.linkedin.com/in/ivapopova?trk=profile-badge'>Iva E. Popova on
                                LinkedIn</a></div>
                    </div>
                </section>
            </section>


            <section class="main-sesction-title" id="IntroductionToNLP"><h1>Introduction to Natural Language Processing (NLP)</h1></section>
            <section class="sub-sections"><h2>What is NLP?</h2>
                <section>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; align-items: center; gap: 1em;">
                        <div style="text-align: center;">
                            <img src="./images/WhatIsNLP.png" alt="What is NLP?" style="height: 80vh; max-width: 100%; object-fit: contain;">
                        </div>
                        <div>
                            <dl class="fa">
                                <dt>Natural Language Processing (NLP) is a branch of Artificial Intelligence that helps computers <strong>understand</strong>, <strong>interpret</strong>, and <strong>generate</strong> human language.</dt>
                                <dt>Think of it as teaching computers to "read" and "write" the way humans do.</dt>
                            </dl>
                        </div>
                    </div>
                </section>
            </section>
            <section class="sub-sections"><h2>Real-world applications of NLP</h2>
                <section>
                     <dl class="fa" style="min-width:80vw">
                        <dt><strong>Virtual assistants</strong>: Siri, Alexa, Google Assistant</dt>
                        <dt><strong>Email spam filters</strong>: detecting unwanted emails</dt>
                        <dt><strong>Autocomplete and autocorrect</strong>: your phone predicting what you'll type next</dt>
                        <dt><strong>Machine translation</strong>: Google Translate converting between languages</dt>
                        <dt><strong>Sentiment analysis</strong>: companies analyzing customer reviews</dt>
                        <dt><strong>Chatbots</strong>: customer service bots on websites</dt>
                    </dl>
                </section>
            </section>
            <section class="sub-sections"><h2>Why is NLP Challenging?</h2>
                <section id="Ambiguity"><h3>Ambiguity</h3>
                    <dl class="fa" style="min-width:80vw">
                       <dt>Words or sentences can have multiple meanings.</dt>
                       <ul>
                            <li><strong>Lexical ambiguity:</strong>
                            <br>"Bank" → financial institution or riverbank.</li>
                            <li><strong>Syntactic ambiguity:</strong>
                            <br>"I saw the man with the telescope."
                            <br>→ Who has the telescope?</li>
                       </ul>
                    </dl>
                </section>
                <section id="ContextDependence"><h3>Context Dependence</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Meaning often depends on context.</dt>
                        <ul>
                            <li><i>"It’s cold in here."</i> might mean a complaint, a request to close the window, or just an observation.</li>
                            <li><i>"Apple"</i> could refer to the fruit or the company depending on the conversation.</li>
                        </ul>
                    </dl>
                </section>
                <section id="IdiomsFigurativeLanguage"><h3>Idioms and Figurative Language</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Expressions rarely mean what the words literally say.</dt>
                        <ul>
                            <li><i>"Kick the bucket"</i> = die</li>
                            <li><i>"Break the ice"</i> = make people comfortable</li>
                            <li><i>"Spill the tea"</i> = share gossip</li>
                        </ul>
                    </dl>
                </section>
                <section id="SarcasmTone"><h3>Sarcasm and Tone</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Machines struggle because the literal meaning is often the opposite.</dt>
                        <ul>
                            <li><i>"Great job…"</i> (said after a mistake)</li>
                            <li><i>"Yeah, that’s exactly what I wanted."</i> (frustrated tone)</li>
                        </ul>
                    </dl>
                </section>
                <section id="UnstructuredNoisyInput"><h3>Unstructured and Noisy Input</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Real-world text is messy.</dt>
                        <ul>
                            <li><i>"definately"</i></li>
                            <li><i>"That movie was fire."</i></li>
                            <li><i>"idk lol u ok?"</i></li>
                        </ul>
                    </dl>
                </section>
                <section id="VariabilityExpression"><h3>Variability in Expression</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>There are many ways to express the same idea.</dt>
                        <ul>
                            <li><i>"I’m hungry."</i></li>
                            <li><i>"I could eat."</i></li>
                            <li><i>"My stomach is yelling at me."</i></li>
                        </ul>
                    </dl>
                </section>
            </section>

            <section class="main-sesction-title" id="The_NLPPipeline"><h1>The NLP Pipeline</h1></section>
            <section class="sub-sections"><h2>The NLP Pipeline</h2>
                <section>
                    <dl class="fa" style="min-width:80vw">
                        <dt>When we process text, we typically follow these steps:</dt>
                        <dd>Raw Text → Preprocessing → Tokenization → Vectorization → Model → Evaluation → Deployment</dd>
                    </dl>
                </section>
            </section>
            <section class="sub-sections"><h2>Text Preprocessing: Cleaning and Normalizing Text</h2>
                <section id="Lowercasing"><h3>Lowercasing</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Convert all text to lowercase to standardize the data.</dt>
                        <dt>Example:</dt>
                        <dd>Converting all text to lowercase ensures "Hello", "hello", and "HELLO" are treated the same.</dd>
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            text = "Hello World! How are YOU?"
                            cleaned = text.lower()
                            print(cleaned)  # "hello world! how are you?"
                        </code></pre>
                    </dl>
                </section>
                <section id="RemovingPunctuation"><h3>Removing Punctuation</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Punctuation often doesn't add meaning for basic tasks.</dt>
                        <dt>Example:</dt>
                        <dd>Removing punctuation ensures "Hello, world!" and "Hello world" are treated the same.</dd>
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            text = "Hello, world! How are you?"

                            # Remove all punctuation
                            punctuations = ["!", "?", ",", "."]
                            cleaned = "".join([char for char in text if char not in punctuations])

                            print(cleaned)  # "Hello world How are you"
                        </code></pre>
                    </dl>
                </section>
                <section id="RemovingNumbers"><h3>Removing Numbers</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Sometimes numbers aren't relevant to our analysis.</dt>
                        <dt>Example:</dt>
                        <dd>Removing numbers ensures "I bought 3 apples for $5.99" and "I bought apples for $" are treated the same.</dd>
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            import re

                            text = "I bought 3 apples for $5.99"
                            cleaned = re.sub(r'\d+', '', text)
                            print(cleaned)  # "I bought  apples for $."
                        </code></pre>
                    </dl>
                </section>
                <section id="RemovingStopwords"><h3>Removing Stopwords</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Stopwords are common words that carry little meaning: "the", "is", "at", "which", "a", etc.</dt>
                        <dt>For next example, we will use <a href="https://spacy.io/">spaCy</a> - a powerful library for natural language processing (NLP) in Python. You can install it using <code>pip install spacy</code>. </dt>
                        <dt>The model for English language must be installed first (run once):</dt>
                        <dd><code>python -m spacy download en_core_web_sm</code></dd>
                        <pre><code rel="Python" class="python" style="min-height: 70vh;">
                            import spacy

                            # Load the spaCy English model.
                            nlp = spacy.load("en_core_web_sm")

                            text = "This is a sample sentence showing off stopword removal."

                            # Process the text to create a Doc object.
                            doc = nlp(text)

                            # Filter the tokens based on their spaCy properties.
                            # - token.text: The actual word string
                            # - token.is_stop: True if the token is a stop word (e.g., 'is', 'a', 'off')
                            # - token.is_punct: True if the token is a punctuation mark (e.g., '.')
                            # - token.is_space: True if the token is only whitespace
                            filtered_tokens = [
                                token.text
                                for token in doc
                                if not token.is_stop and not token.is_punct and not token.is_space
                            ]

                            # Output the result
                            print(f"Original Text: {text}")
                            print(f"Filtered Tokens: {filtered_tokens}")
                        </code></pre>
                    </dl>
                </section>
                <section id="StemmingLemmatization"><h3>Stemming and Lemmatization</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Both stemming and lemmatization are techniques used in NLP to reduce words to their base or root form</dt>
                        <dt><strong>Stemming</strong> is a crude heuristic process that lops off the ends of words, usually generating a "stem" that is often not a real word</dt>
                        <dd>running → run, better → better</dd>
                        <dt><strong>Lemmatization</strong> is a more advanced process that uses the language's rules to reduce words to their base or root form</dt>
                        <dd>running → run, better → good</dd>
                        <dt>Example:</dt>
                        <pre><code rel="Python" class="python" style="min-height: 70vh;">
                            import spacy

                            # Load the small English model
                            nlp = spacy.load("en_core_web_sm")

                            text = (
                                "The scientists are running several new experiments. "
                                "He runs very easily and fairly well. "
                                "She quickly ran from the room."
                            )

                            # Process the text to create a Doc object with tokens, POS tags, etc.
                            doc = nlp(text)

                            print("Original Text:\n", text)
                            print("-" * 50)

                            # Extract and print the token, its Part-of-Speech (POS), and its Lemma
                            print(&quot;{:&lt;12} {:&lt;6} {:&lt;12}&quot;.format(&quot;Token&quot;, &quot;POS&quot;, &quot;Lemma&quot;))
                            print(&quot;{:&lt;12} {:&lt;6} {:&lt;12}&quot;.format(&quot;-----&quot;, &quot;---&quot;, &quot;-----&quot;))

                            results = []
                            for token in doc:
                                # We skip spaces and punctuation for a cleaner output of only words
                                if token.is_punct or token.is_space:
                                    continue

                                # token.text: The original word
                                # token.pos_: The Part-of-Speech tag (e.g., VERB, ADV, NOUN)
                                # token.lemma_: The base form of the word
                                results.append({"token": token.text, "pos": token.pos_, "lemma": token.lemma_})

                                # Print formatted output
                                print(&quot;{:&lt;12} {:&lt;6} {:&lt;12}&quot;.format(token.text, token.pos_, token.lemma_))

                        </code></pre>
                    </dl>
                </section>
            </section>
            <section class="sub-sections"><h2>Tokenization</h2>
                <section id="WhatIsTokenization"><h3>What is Tokenization?</h3>
                    <dl class="fa">
                        <dt>Tokenization is the fundamental first step in processing text for any Natural Language Processing (NLP) or Large Language Model (LLM) task.</dt>
                        <dt>It is the process of breaking down a stream of text into smaller, meaningful units called "tokens".</dt>
                        <dt>A <strong>token</strong> can be a single word, a part of a word (a subword), characters, numbers, or any significant element in the text, depending on the type of tokenizer used</dt>
                        <dt>For example, if the input is: <i>"Don't stop!"</i>, the resulting tokens might be ["Don", "'t", "stop", "!"]</dt>
                        <dd>Reference: <a href="https://platform.openai.com/tokenizer">OpenAI Tokenizer</a></dd>
                        <dt>There is no single "right" way to tokenize text. While the goal is the same—to segment text—the methodology varies widely</dt>
                    </dl>
                </section>
                <section id="WordTokenization"><h3>Word Tokenization (Traditional NLP)</h3>
                    <dl class="fa">
                        <dt>This is the simplest form. It breaks text into tokens based on delimiters, typically spaces and punctuation.</dt>
                        <dt>Process: Splits on whitespace, then handles punctuation.</dt>
                        <dt>Example: "I am running." → ['I', 'am', 'running', '.']</dt>
                        <dt>Drawback: Struggles with contractions (don't), hyphenated words (state-of-the-art), and different languages.</dt>
                    </dl>
                </section>
                <section id="WordTokenizationSpaCy"><h3>Word Tokenization - Example with spaCy</h3>
                    <dl class="fa">
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            import spacy

                            nlp = spacy.load("en_core_web_sm")
                            text = "NLP is amazing! Let's learn it."
                            doc = nlp(text)

                            tokens = [token.text for token in doc]
                            print(tokens)
                            # Output: ['NLP', 'is', 'amazing', '!', 'Let', "'s", 'learn', 'it', '.']
                        </code></pre>
                        <dt>This treats each “word” (and punctuation) as a token. This is often simpler and more intuitive. As explained in spaCy docs, tokenization is rule-based (splitting on whitespace and punctuation, handling contractions, etc.) and is “non-destructive” (you can reconstruct the original text).</dt>
                        <dt>Reference: <a href="https://spacy.io/usage/linguistic-features#tokenization">spaCy Tokenization</a></dt>
                    </dl>
                </section>
                <section id="SubwordTokenization"><h3>Subword Tokenization (Modern LLMs)</h3>
                    <dl class="fa">
                        <dt>This is the dominant method used by models like GPT, BERT, and T5. It finds a balance between word and character tokenization by breaking rare words into smaller, frequently occurring units (subwords).</dt>
                        <dt><strong>Process</strong>: Uses statistical methods (like Byte-Pair Encoding (BPE) or WordPiece) to build a vocabulary of common subword units.</dt>
                        <dt><strong>Advantages</strong>:</dt>
                        <dd><strong>Manages Vocabulary</strong>: Handles large datasets efficiently without a massive vocabulary size.</dd>
                        <dd><strong>Handles Unknown Words</strong>: Can represent a new, unseen word (like a misspelled name) by breaking it down into known subwords.</dd>
                        <dd><strong>Handles Morphology</strong>: Implicitly handles stemming/lemmatization by splitting inflected words (e.g., "running" → "run" + "##ning").</dd>
                    </dl>
                </section>
                <section id="SubwordTokenizationBert"><h3>Subword Tokenization - Example with BertTokenizer</h3>
                    <dl class="fa">
                        <dt>In next example we will use a pretrained subword tokenizer: <a href="https://huggingface.co/docs/transformers/quicktour">BertTokenizer</a> from Hugging Face Transformers library — which uses the WordPiece algorithm.</dt>
                        <dt>Install the transformers Library: <a href="https://huggingface.co/docs/transformers/installation">Hugging Face Transformers Installation</a> and the <a href="https://pytorch.org/get-started/locally/">PyTorch</a> framework that it depends on.</dt>
                        <pre><code rel="Terminal" class="bash" style="min-height: 1vh;">
                            # Install transformers library
                            pip install transformers

                            # Install torch library
                            pip install torch
                        </code></pre>
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            from transformers import BertTokenizer

                            tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
                            text = "Tokenization is essential for NLP."
                            subword_tokens = tokenizer.tokenize(text)
                            print("Subword tokens:", subword_tokens)

                            # OUTPUT:
                            # Subword tokens: ['token', '##ization', 'is', 'essential', 'for', 'nl', '##p', '.']
                        </code></pre>
                    </dl>
                </section>
                <section id="WhyTokenizationMatters"><h3>Why Tokenization Matters?</h3>
                    <dl class="fa">
                        <dt>The choice of tokenization strategy directly impacts how a model learns and generalizes. Different tokenization affects the final output because it changes the size of the vocabulary, how out-of-vocabulary (OOV) words are handled, and how input text is segmented.</dt>
                        <dt>Here are examples illustrating how Subword (BPE/WordPiece) and Word Tokenization affect model performance in handling Out-Of-Vocabulary (OOV) Words, like processing a new, unseen technical term or a typo:</dt><br>
                        <table border="1" style="display: block; width: 100%; border-collapse: collapse;">
                            <thead>
                                <tr>
                                <th style="border: 1px solid black;">Tokenization Strategy</th>
                                <th style="border: 1px solid black;">Input Text</th>
                                <th style="border: 1px solid black;">Tokens Generated</th>
                                <th style="border: 1px solid black;">Impact on Model Performance</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                <td style="border: 1px solid black;"><strong>Word Tokenization</strong></td>
                                <td style="border: 1px solid black;">The cryptotoken failed.</td>
                                <td style="border: 1px solid black;">['The', 'cryptotoken', 'failed', '.']</td>
                                <td style="border: 1px solid black;">
                                    The word "cryptotoken" is not in the model's vocabulary. It is replaced with an
                                    <code>&lt;UNK&gt;</code> (Unknown) token. The model completely loses the semantic meaning of that word.
                                </td>
                                </tr>
                                <tr>
                                <td style="border: 1px solid black;"><strong>Subword Tokenization (BPE)</strong></td>
                                <td style="border: 1px solid black;">The cryptotoken failed.</td>
                                <td style="border: 1px solid black;">['The', 'crypto', '##token', 'failed', '.']</td>
                                <td style="border: 1px solid black;">
                                    The tokenizer breaks the word into known subword pieces: <strong>crypto</strong> (known) and <strong>##token</strong> (known).
                                    The model retains most of the word's semantic meaning and can infer its purpose.
                                </td>
                                </tr>
                            </tbody>
                        </table>
                    </dl>
                </section>
            </section>
            <section class="sub-sections"><h2>Text Vectorization</h2>
                <section id="TextVectorizationOverview"><h3>Overview</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Computers don't understand text - they work with numbers.</dt>
                        <dt>Vectorization converts text into numerical representations (vectors or arrays of numbers)</dt>
                    </dl>
                </section>
                <section id="BoW"><h3>Bag of Words (BoW) approach</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>This is the simplest form of vectorization. It converts text into a fixed-size vector based on <strong>word frequency</strong>.</dt>
                        <dt>Concepts:</dt>
                        <div style="font-size: .8em; margin-left: 2em;">
                            <div><b>Document 1</b>: "I love dogs"</div>
                            <div><b>Document 2</b>: "I adore cats"</div>

                            <div style="margin: 1em 0;"><b>Vocabulary</b>: ['adore' 'cats' 'dogs' 'love']</div>

                            <div><b>Vector 1</b>: [0 0 1 1]</div>
                            <div><b>Vector 2</b>: [1 1 0 0]</div>
                        </div>
                        <dt>Example:</dt>
                        <dd>Converting "I love cats" and "I adore cats" to [0 0 1 1] and [1 1 0 0] respectively.</dd>
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            from sklearn.feature_extraction.text import CountVectorizer

                            # the corpus is a list of documents
                            corpus = ["I love dogs", "I adore cats"]

                            # the vectorizer is a tool that converts text into a matrix of token counts
                            vectorizer = CountVectorizer()
                            X = vectorizer.fit_transform(corpus)

                            # the feature names are the vocabulary of the corpus
                            print(vectorizer.get_feature_names_out())

                            # Convert the sparse matrix X to a dense NumPy array
                            print(X.toarray())
                        </code></pre>
                    </dl>
                </section>
                <section id="LimitationsOfBoW"><h3>Limitations of BoW:</h3>
                    <dl class="fa">
                        <dt>Ignores word order: "dog bites man" = "man bites dog"</dt>
                        <dt>Ignores word frequency across documents</dt>
                        <dt>Creates huge, sparse matrices</dt>
                    </dl>
                </section>
                <section id="SparseVsDense"><h3 class="advanced">Concepts clarification: sparse vs dense matrix</h3>
                    <dl class="fa">
                        <dt>Sparse matrix:</dt>
                        <dd><b>Definition:</b> A matrix where most of the elements are zero.</dd>
                        <dd><b>Storage:</b> It only stores the location (row, column) and value of the non-zero elements. It ignores the zeros entirely.</dd>
                        <dd><b>Why use it?</b> For a vocabulary of 50,000 words and 10,000 documents, most documents only use a tiny fraction of the vocabulary. Storing all those zeros would waste enormous amounts of memory and processing power.</dd>

                        <dt>Dense matrix:</dt>
                        <dd><b>Definition:</b> A matrix where every element is stored explicitly, even if the value is zero.</dd>
                        <dd><b>Storage:</b> Stores every element in contiguous memory blocks.</dd>
                        <dd><b>Why use it?</b> Required for compatibility with many general-purpose functions, easy visualization (like converting to a pandas DataFrame), and traditional mathematical operations.</dd>
                    </dl>
                </section>
                <section id="TF-IDF"><h3>TF-IDF (Term Frequency - Inverse Document Frequency)</h3>
                    <dl class="fa">
                        <dt>TF-IDF improves on BoW by considering how important a word is to a document relative to all documents.</dt>
                        <dd>TF (Term Frequency): How often word appears in document</dd>
                        <dd>IDF (Inverse Document Frequency): How rare the word is across all documents</dd>
                        <dt>Formula: TF-IDF = TF × IDF</dt>
                        <dd>Words that appear often in one document but rarely in others get high scores.</dd>
                        <dd>Words that appear often in all documents get low scores.</dd>
                    </dl>
                </section>
                <section><h3>TF-IDF Example</h3>
                    <pre><code rel="Python" class="python" style="min-height: 100vh;">
                        from sklearn.feature_extraction.text import TfidfVectorizer
                        import pandas as pd

                        # Define the corpus
                        documents = [
                            "I love machine learning",
                            "I love coding",
                            "Machine learning is amazing",
                            "Deep learning is a subset of machine learning",
                        ]

                        # Create TF-IDF vectorizer
                        tfidf_vectorizer = TfidfVectorizer()

                        # Fit the vectorizer to the documents and transform the documents into the TF-IDF matrix
                        # The result is a sparse matrix (efficient for large datasets)
                        tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

                        # Get the feature names (vocabulary)
                        vocabulary = tfidf_vectorizer.get_feature_names_out()

                        ### Convert the sparse TF-IDF matrix to a DataFrame
                        # Convert to a dense NumPy array and round the scores for readability
                        tfidf_array = tfidf_matrix.toarray().round(4)

                        # Define descriptive indices for the documents (rows)
                        document_indices = [f"Doc {i + 1}" for i in range(len(documents))]

                        # Create a DataFrame
                        df_tfidf = pd.DataFrame(tfidf_array, columns=vocabulary, index=document_indices)

                        print("## TF-IDF Matrix as DataFrame")
                        print("-----------------------------")
                        print(df_tfidf)

                        # Optional: Print the original documents for easy reference
                        print("\n## Original Documents")
                        print("---------------------------------")
                        for i, doc in enumerate(documents):
                            print(f"Doc {i + 1}: {doc}")

                    </code></pre>
                </section>
                <section><h3>Limitations of TF-IDF</h3>
                    <dl class="fa">
                        <dt>TF-IDF ignores word order</dt>
                        <dt>TF-IDF doesn't capture semantic meaning</dt>
                        <dt>TF-IDF can be sensitive to stop words</dt>
                    </dl>
                </section>
                <section><h3>When to use TF-IDF</h3>
                    <dl class="fa">
                        <dt>Text classification</dt>
                        <dt>Information retrieval</dt>
                        <dt>Document similarity</dt>
                        <dt>When rare words are more important than common ones</dt>
                    </dl>
                </section>
                <section><h3>Word Embeddings</h3>
                    <dl class="fa">
                        <dt>Represent words as dense vectors in continuous space where similar words are close together.</dt>
                        <dt><i><strong>Key idea:</i> Words with similar meanings have similar vectors.</strong></dt>
                    </dl>
                    <a href="./images/WordEmbeddings.png"><img src="./images/WordEmbeddings.png" alt="Word Embeddings" style="height:50vh"></a>
                    <p>Reference: <a href="https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html">www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html</a></p>
                </section>
                <section><h3>Popular Word Embedding Models</h3>
                    <dl class="fa">
                        <dt><strong>Word2Vec</strong>: Google's model (2013)</dt>
                        <dt><strong>GloVe</strong>: Stanford's model</dt>
                        <dt><strong>FastText</strong>: Facebook's model (handles unknown words better)</dt>
                        <dt><strong>Modern contextual embeddings</strong>: BERT, GPT</dt>
                    </dl>
                </section>
                </section>
                <section><h3>Word Embeddings - Example</h3>
                    <dl class="fa">
                        <dt>Next example uses <a href="https://radimrehurek.com/gensim/models/word2vec.html">gensim</a> library</dt>
                    </dl>
                    <pre><code rel="Python" class="python" style="min-height: 100vh;">
                        from gensim.models import Word2Vec

                        ## Training Data - list of tokenized sentences
                        sentences = [
                            ["machine", "learning", "is", "fun"],
                            ["deep", "learning", "is", "powerful"],
                            ["machine", "learning", "uses", "algorithms"],
                            ["deep", "learning", "uses", "neural", "networks"],
                        ]

                        ## Train Word2Vec model
                        model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=4)
                        # The hyperparameter settings are:
                        # vector_size=50: Sets the dimensionality of the resulting word vectors. Every word will be represented by a 50-element vector (instead of the standard 300 for large models).
                        # window=3: Sets the context window size. When training, the model considers words up to 3 positions away from the target word to be its context.
                        # min_count=1: Ignores all words with a total frequency lower than this. Since it's set to 1, all words in our small dataset are included in the final vocabulary.
                        # workers=4: Uses 4 parallel threads (CPU cores) to speed up the training process.

                        ## Get vocabulary
                        # model.wv is a dictionary-like object which holds the trained embeddings (word vectors)
                        vocabulary = model.wv.index_to_key
                        print(f"Vocabulary: {vocabulary}")

                        # Get vector for a word
                        vector = model.wv["learning"]
                        print(f"Vector for 'learning': {vector[:5]}...")  # Show first 5 dimensions

                        # Find similar words
                        similar_words = model.wv.most_similar("learning", topn=3)
                        print(f"\nWords similar to 'learning': {similar_words}")
                    </code></pre>
                </section>
            </section>

            <section class="main-sesction-title" id="BasicNLPTasks"><h1>Basic NLP Tasks</h1></section>
            <section class="sub-sections"><h2>Text Classification</h2>
                <section><h3>Overview</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Text classification is the task of assigning a label or class to a piece of text.</dt>
                        <dt>It is a fundamental task in NLP and has a wide range of applications.</dt>
                        <dt>Examples:</dt>
                        <dd>Spam detection (spam/not spam)</dd>
                        <dd>Sentiment analysis (positive/negative/neutral)</dd>
                        <dd>Topic classification (sports/politics/tech)</dd>
                    </dl>
                </section>
            </section>
            <section class="sub-sections"><h2>Named Entity Recognition (NER)</h2>
                <section><h3>Overview</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Named Entity Recognition (NER) is the task of identifying and classifying named entities in text.</dt>
                        <dt>Example: Let's have the sentence <i>"Apple Inc. is planning to open a new store in New York next month."</i>. We can extract the following entities:</dt>
                        <dd>Apple Inc. is an organization (ORG)</dd>
                        <dd>New York is a geopolitical entity (GPE)</dd>
                        <dd>next month is a date (DATE)</dd>
                    </dl>
                </section>
                <section><h3>Example</h3>
                    <dl class="fa">
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            import spacy

                            # Load English model
                            nlp = spacy.load("en_core_web_sm")

                            text = "Apple Inc. is planning to open a new store in New York next month."
                            doc = nlp(text)

                            print("Named Entities:")
                            for ent in doc.ents:
                                print(f"{ent.text}: {ent.label_}")
                            # Apple Inc.: ORG (Organization)
                            # New York: GPE (Geopolitical Entity)
                            # next month: DATE
                        </code></pre>
                    </dl>
                </section>
            </section>
            <section class="sub-sections"><h2>Part-of-Speech (POS) Tagging</h2>
                <section>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Part-of-Speech (POS) Tagging is the task of identifying and classifying the grammatical roles of words in text.</dt>
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            import spacy

                            # Load English model
                            nlp = spacy.load("en_core_web_sm")

                            text = "The quick brown fox jumps over the lazy dog"

                            doc = nlp(text)
                            for token in doc:
                                print(f"{token.text}: {token.pos_}")
                        </code></pre>
                    </dl>
                </section>
            </section>
            <section class="sub-sections"><h2>Text Similarity</h2>
                <section>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Text Similarity is the task of measuring the similarity between two pieces of text.</dt>
                        <pre><code rel="Python" class="python" style="min-height: 1vh;">
                            from sklearn.metrics.pairwise import cosine_similarity
                            from sklearn.feature_extraction.text import TfidfVectorizer

                            texts = [
                                "I love machine learning",
                                "I enjoy machine learning",
                                "The weather is nice today"
                            ]

                            vectorizer = TfidfVectorizer()
                            vectors = vectorizer.fit_transform(texts)

                            similarity = cosine_similarity(vectors[0:1], vectors[1:2])
                            print(f"Similarity between text 1 and 2: {similarity[0][0]:.3f}")

                            similarity = cosine_similarity(vectors[0:1], vectors[2:3])
                            print(f"Similarity between text 1 and 3: {similarity[0][0]:.3f}")
                        </code></pre>
                    </dl>
                </section>
            </section>


            <section class="main-section-title"><h1>Classification Metrics</h1></section>
            <section class="sub-sections"><h2>Classification Metrics</h2>
                <section><h3>Accuracy</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Accuracy is the ratio of correctly predicted instances to the total instances.</dt>
                        <dt>Accuracy gives us the percentage of correct predictions</dt>
                    </dl>
                </section>
                <section><h3>Confusion Matrix</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>A confusion matrix is a table used to evaluate the performance of a classification model.</dt>
                        <dt>It helps you understand whether it confuses positive and negatives, helping you improve its accuracy.</dt>
                        <div style="text-align: center;">
                          <img src="./images/confusion_matrix.png" alt="Confusion Matrix" style="width: 50%;">
                        </div>
                        <dt>True Positive (TP): The model correctly predicts the positive class.</dt>
                        <dt>True Negative (TN): The model correctly predicts the negative class.</dt>
                        <dt>False Positive (FP): The model incorrectly predicts the positive class (Type I error).</dt>
                        <dt>False Negative (FN): The model incorrectly predicts the negative class (Type II error).</dt>
                    </dl>
                </section>
                <section><h3>Precision and recall</h3>
                    <dl class="fa" style="min-width:80vw">
                        <dt>Precision is the ratio of correctly predicted positive observations to the total predicted positives.</dt>
                        <dd>Precision = TP / (TP + FP)</dd>
                        <dt>Recall (Sensitivity) is the ratio of correctly predicted positive observations to all actual positives.</dt>
                        <dd>Recall = TP / (TP + FN)</dd>
                        <div style="display: flex; justify-content: space-around; align-items: flex-start;">
                          <img src="./images/Precision_Recall_part1.png" alt="Precision_Recall_part1" style="width: 40%;">
                          <img src="./images/Precision_Recall_part2.png" alt="Precision_Recall_part2" style="width: 40%; margin-top: 4em;">
                        </div>
                    </dl>
                </section>
            </section>

            <section class="main-section-title"><h1>Practical example: Simple Sentiment Analysis model</h1></section>
            <section class="sub-sections"><h2>Practical example: Simple Sentiment Analysis model</h2>
                <section>
                    <dl class="fa" style="min-width:80vw">
                        <dt>The project is given in next <a href="https://github.com/geekcourses/ProgressBG-ChatGPT_and_ML-Slides/blob/main/pages/themes/NLP_Basics/examples/SentimentAnalysis/PracticalExample-SimpleSentimentAnalysisModel.ipynb">PracticalExample-SimpleSentimentAnalysisModel.ipynb</a> file.</dt>
                        <dd>You can view it at <a href="https://nbviewer.org/github/geekcourses/ProgressBG-ChatGPT_and_ML-Slides/blob/main/pages/themes/NLP_Basics/examples/SentimentAnalysis/PracticalExample-SimpleSentimentAnalysisModel.ipynb">PracticalExample-SimpleSentimentAnalysisModel @nbviewer</a>.</dd>
                    </dl>
                </section>
            </section>





            <section class="disclaimer"
                data-background="/ProgressBG-ChatGPT_and_ML-Slides/outfit/images/for_slides/the_end_on_sand.jpg">
                <p>These slides are based on</p>
                <p>customised version of </p>
                <p><a href="http://hakim.se/">Hakimel</a>'s <a href="http://lab.hakim.se/reveal-js">reveal.js</a></p>
                <p>framework</p>
            </section>
            <!--
########################################################
##################### SLIDES END   #####################
########################################################
-->
        </div>
    </div>
    <!-- Custom processing -->
    <script src="/ProgressBG-ChatGPT_and_ML-Slides/outfit/js/slides.js"></script>
    <!-- external scripts -->
    <script src="/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/lib/js/head.min.js"></script>
    <script src="/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/js/reveal.js"></script>
    <!-- init reveal -->
    <script>
        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        var highlightjsTabSize = '  ';
        Reveal.initialize({
            controls: true,
            progress: true,
            slideNumber: 'c/t',
            keyboard: true,
            history: true,
            center: true,
            width: 1920,
            height: 1280,
            // Bounds for smallest/largest possible scale to apply to content
            // minScale: .5,
            maxScale: 1,
            // slide transition
            transition: 'concave', // none/fade/slide/convex/concave/zoom
            // Factor of the display size that should remain empty around the content
            margin: 0.1,
            // shift+mouse click to zoom in/out element
            zoomKey: 'ctrl',
            // theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
            // transition: Reveal.getQueryHash().transition || 'default'
            // Optional reveal.js plugins
            dependencies: [
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/lib/js/classList.js', condition: function () { return !document.body.classList; } },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/highlight/highlight.js', async: true, callback: function () { hljs.configure(); hljs.initHighlightingOnLoad(); } },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/zoom-js/zoom.js', async: true },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/notes/notes.js', async: true }
            ]
        });
    </script>
</body>

</html>