{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df602473",
   "metadata": {},
   "source": [
    "# Build a RAG with Langchain and ChromaDB\n",
    "\n",
    "To build this RAG system, we will use a \"hybrid\" approach: the HuggingFace model runs locally on your machine to create embeddings, while the Gemini API is called over the web to generate the final response.\n",
    "\n",
    "## Install Dependencies\n",
    "\n",
    "We need to install the specific integration packages for LangChain to talk to Google (Gemini), HuggingFace, and ChromaDB.\n",
    "\n",
    "Activate your project virtual environment and install:\n",
    "```\n",
    "pip install langchain langchain-google-genai langchain-huggingface langchain-chroma\n",
    "```\n",
    "\n",
    "## Initialize the Embedding Model\n",
    "\n",
    "We will use a popular HuggingFace model (all-MiniLM-L6-v2) because it is lightweight and runs fast on a standard CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a08197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized locally.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the local HuggingFace embedding model\n",
    "# This model will download automatically on first run (~80MB)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Embedding model initialized locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6583de9",
   "metadata": {},
   "source": [
    "## Setup Gemini and ChromaDB\n",
    "\n",
    "Now we set up the \"Generator\" (Gemini) and our \"Knowledge Base\" (ChromaDB). You will need your Google AI Studio API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632e14ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System ready.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# 1. Set your API Key\n",
    "# Load the variables from .env into the environment\n",
    "load_dotenv(find_dotenv())\n",
    "# Access the key using os.environ\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# 2. Initialize Gemini 2.0 (The LLM)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "\n",
    "# 3. Create a tiny local database with some example data\n",
    "docs = [\n",
    "    Document(page_content=\"Gemini 2.0 was released in late 2024 with improved multimodal capabilities.\"),\n",
    "    Document(page_content=\"RAG stands for Retrieval-Augmented Generation, a technique to ground LLMs in external data.\"),\n",
    "]\n",
    "\n",
    "# This creates an in-memory ChromaDB using our HF embeddings\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "\n",
    "print(\"System ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84f82b",
   "metadata": {},
   "source": [
    "## Create the Retrieval Chain\n",
    "\n",
    "We use the LangChain Expression Language (LCEL) to pipe the different components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed38be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is RAG?\n",
      "AI Response: RAG stands for Retrieval-Augmented Generation, a technique to ground LLMs in external data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Define the Prompt Template (The \"Instructions\")\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 2. Setup the Retriever (The \"Search Engine\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# 3. Build the Chain\n",
    "# This chain: Finds data -> formats the prompt -> sends to Gemini -> cleans the output\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 4. Run the example\n",
    "query = \"What is RAG?\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"AI Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4b0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dae9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
