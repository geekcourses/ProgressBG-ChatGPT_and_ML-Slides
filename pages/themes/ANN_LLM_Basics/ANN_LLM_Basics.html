<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>IntroductionToLLM</title>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <link rel="shortcut icon" href="/ProgressBG-ChatGPT_and_ML-Slides/images/favicons/favicon-32.png">
    <!-- css & themes include -->
    <link rel="stylesheet" href="/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/css/reveal.css">
    <link rel="stylesheet" href="/ProgressBG-ChatGPT_and_ML-Slides/outfit/css/themes/light.css" id="theme">
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/css/print/pdf.css' : '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

    <!-- add MathJax support -->
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <base target="_blank">
</head>
<body>
    <div class="reveal default center" data-transition-speed="default" data-background-transition="default">
        <div class="top_links">
            <a class="home_link" href="/ProgressBG-ChatGPT_and_ML-Slides/pages/agenda/agenda.html#IntroductionToLLM" target="_top"><i class="fa fa-home"></i></a>
            <span class="help_link" href="#"><i class="fa fa-question"></i></span>
            <div class="help_text">
                <div class="note">Keyboard shortcuts:</div>
                <div><span>N/Space</span><span>Next Slide</span></div>
                <div><span>P</span><span>Previous Slide</span></div>
                <div><span>O</span><span>Slides Overview</span></div>
                <div><span>ctrl+left click</span><span>Zoom Element</span></div>
                <div class="print-howto"><br>If you want print version => add '<code>?print-pdf</code>' <br> at the end of slides URL (remove '#' fragment) and then print. <br>
                Like: https://ProgressBG-ChatGPT_and_ML-course.github.io/...CourseIntro.html?print-pdf </div>
            </div>
        </div>
        <div class="footer theme_switch">
            <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ProgressBG-ChatGPT_and_ML-Slides/outfit/css/themes/dark.css'); return false;">Dark</a>
            <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ProgressBG-ChatGPT_and_ML-Slides/outfit/css/themes/light.css'); return false;">Light</a>
            <a href="#" onclick="document.getElementById('theme').setAttribute('href','/ProgressBG-ChatGPT_and_ML-Slides/outfit/css/themes/projector.css'); return false;">Projector</a>
        </div>
        <div class="slides">
<!--
########################################################
##################### SLIDES START #####################
########################################################
-->
<section id="Introduction"><h1>Introduction to ANN, LLM and ChatGPT</h1></section>
<section data-transition="zoom">
    <!-- linkedin badge -->
    <!--<script type="text/javascript" src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>-->
    <section class="copyright" data-transition="zoom">
        <div class="note">
            <p>Created for</p>
        </div>
        <div class="company">
            <a href="http://progressbg.net/Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¸Ñ€Ð°Ð½Ðµ-Ñ-python-2/">
            <img style="height:80%" src="/ProgressBG-ChatGPT_and_ML-Slides/outfit/images/logos/ProgressBG_logo_529_127.png">
            </a>
        </div>
        <div class="author">
            <span class="note">Iva E. Popova, 2016-2025,</span>
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png"></a>
        </div>
    </section>
    <section class="copyright" data-transition="zoom" style="margin-top: -2em;">
        <div class="company">
             <div class="LI-profile-badge"  data-version="v1" data-size="large" data-locale="en_US" data-type="vertical" data-theme="dark" data-vanity="ivapopova"><a class="LI-simple-link" href='https://bg.linkedin.com/in/ivapopova?trk=profile-badge'>Iva E. Popova on LinkedIn</a></div>
        </div>
    </section>
</section>


<section id="Overview" data-min="__Y__" class="main-section-title"><h1>Overview</h1></section>
<section id="WhatIsANN" class="sub-sections"><h2>Overview</h2>
    <section id="ANNDefinition">
        <dl class="fa">
            <dt>Artificial Neural Networks (ANNs) are a subfield of Machine Learning and AI that is inspired by the structure and function of the human brain.</dt>
            <dt>ANNs are composed of a large number of interconnected processing units called neurons, which are organized in layers.</dt>
            <dt>ANNs are used to model complex nonlinear relationships between input and output data.</dt>
        </dl>
        <a href="./images/DL.gif"><img src="./images/DL.gif" alt="" style="height: 50vh;"></a>
    </section>
    <section id="BioVsArtificialNeuron"><h3>The inspiration</h3>
        <dl class="fa">
            <dt>Artificial Neural Networks (ANNs) are inspired by the structure and function of the human brain. The idea of ANNs is based on the concept that the brain is a network of interconnected neurons that communicate with each other to process and transmit information. </dt>
            <dt>Biological neurons communicate through electrochemical signals, while artificial neurons use mathematical functions to process inputs and produce outputs.</dt>
        </dl>
        <a href="./images/bioneuron_vs_artneuron.webp"><img src="./images/bioneuron_vs_artneuron.webp" alt="bioneuron_vs_artneuron" style="height: 80vh; margin-top:2em"></a>
    </section>
    <section id="PerceptronModel"><h3>The Artificial Neuron (Perceptron Model)</h3>
        <a href="./images/Perceptron.png"><img src="./images/Perceptron.png" alt="Perceptron.png"></a>
        <dl class="fa">
            <dt><strong>Input Signals ($\mathbf{x}$):</strong> The neuron receives multiple input data points, $x_1, x_2, \ldots, x_n$, which form the input vector $\mathbf{x}$.</dt>
            <dt><strong>Weights ($\mathbf{w}$):</strong> Each input is multiplied by an adjustable weight, $w_1, w_2, \ldots, w_n$, which is a real-valued number that determines the importance of the feature.</dt>
            <dd>During the training process, the neural network adjusts these weights to assign the correct "importance" to each input feature for the task at hand.</dd>
            <dt><strong>Bias and Summation:</strong> A bias ($w_0 = \theta$) is often included to shift the threshold (or decision boundary), allowing the neuron to activate even when inputs are zero. All weighted inputs and the bias are then summed up (($\Sigma$)).</dt>
            <dt><strong>Activation/Output:</strong> The summed signal is passed through an activation function (like the step function shown) to produce the final output $out(t)$.</dt>
        </dl>
    </section>
</section>

<section id="ANNStructure" data-min="50" class="main-section-title"><h1>ANN Structure</h1></section>
<section id="ArchitectureOfANN" class="sub-sections"><h2>ANN Structure</h2>
    <section id="NetworkGraph">
        <dl class="fa">
            <dt>The structure of an ANN can be described as a directed graph, where each neuron in one layer is connected to every neuron in the next layer. </dt>
            <dt>The input layer is where the data is fed into the network, and the output layer produces the result. </dt>
            <dt>In between, there can be one or more hidden layers, which are used to extract relevant features from the input data.</dt>
            <a href="./images/ANN_Structure.png"><img src="./images/ANN_Structure.png" alt="ANN_Structure.png"></a>
        </dl>
    </section>
    <section id="LayersDefinition"><h3>Layers</h3>
        <dl class="fa">
            <dt>Input layer</dt>
            <dd>This is the layer where the input data is fed into the network.</dd>
            <dd>The number of neurons in the input layer is determined by the size of the input data.</dd>
            <dd>For example, if we are using a neural network for image classification, the input layer will have neurons corresponding to the pixels of the image.</dd>
            <dt>Hidden layer</dt>
            <dd>This is the layer that lies between the input and output layers of the network.</dd>
            <dd>The number of hidden layers and the number of neurons in each hidden layer can vary depending on the complexity of the problem being solved</dd>
            <dd>The hidden layer(s) are used to extract relevant features from the input data</dd>
            <dt>Output layer</dt>
            <dd>This is the final layer of the network that produces the output. </dd>
            <dd>The number of neurons in the output layer is determined by the type of problem being solved. </dd>
            <dd>For example, in a binary classification problem, the output layer will have two neurons corresponding to the two possible classes.</dd>
        </dl>
    </section>

    <section id="DenseLayerKeras"><h3>Example of a Dense Layer in Keras</h3>
        <dl class="fa" style="min-width:80vw">

            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                from keras.models import Sequential
                from keras.layers import Dense

                # Create a simple model with a Dense layer
                model = Sequential()
                # Add a Dense layer with 32 neurons and ReLU activation
                model.add(Dense(32, input_dim=8, activation='relu'))
            </code></pre>

            <dt>The layer contains 32 neurons. Each neuron receives input from every neuron in the previous layer (or the input data).</dt>
            <dt>The layer expects an input of 8 features. This means the input to the network has 8 dimensions or attributes, which are passed to each of the 32 neurons.</dt>
            <dt>Each of the 32 neurons has 8 corresponding weights (one for each input feature) and a bias term.</dt>
            <dd>The total number of trainable parameters for this layer is \( (8 \times 32) + 32 = 288 \) (weights + biases).</dd>
            <dt>The ReLU (Rectified Linear Unit) activation function is applied to the output of each neuron. This introduces non-linearity, making the network capable of learning complex patterns.</dt>
        </dl>
    </section>
</section>

<section id="ActivationFunction" data-min="50" class="main-section-title"><h1>Activation Function</h1></section>
<section id="RoleOfActivationFunctions" class="sub-sections"><h2>Activation Function</h2>
    <section id="ActivationFunctionIntro">
        <dl class="fa">
            <dt>Activation functions are an essential part of Neural Networks as they are responsible for introducing non-linearity into the model. Without activation functions, Neural Networks would essentially be a series of linear operations, which would severely limit the model's capacity to learn complex patterns.</dt>
            <dt>There are several types of activation functions used in ANNs, including sigmoid, ReLU, and tanh. </dt>
            <dt>The choice of activation function depends on the problem being solved and the type of data being processed.</dt>
        </dl>
    </section>
    <section id="SigmoidFunction"><h3>Sigmoid Function</h3>
        <dl class="fa">
            <dt>
                The Sigmoid function is one of the earliest and most widely used activation functions in Neural Networks. The Sigmoid function maps any input value to a value between 0 and 1, which makes it useful in binary classification problems. The mathematical formula for the Sigmoid function is given below:

                $$\\sigma(x) = \frac{1}{1 + e^{-x}}$$

            </dt>
            <dt>The Sigmoid function has the following properties:</dt>
            <dd>It is a smooth, continuous function.</dd>
            <dd>Its output is always between 0 and 1.</dd>
            <dd>The output is centered around 0.5, which can make it problematic in cases where the input values are very large or very small.</dd>
        </dl>
        <img src="./images/sigmoit-plot.png" alt="">
    </section>
    <section id="ReLUFunction"><h3>ReLU Function</h3>
        <dl class="fa">
            <dt>The Rectified Linear Unit (ReLU) function is another popular activation function used in Neural Networks. The ReLU function maps any input value to either 0 or the input value itself, which makes it useful in cases where we want to introduce sparsity in the model. The mathematical formula for the ReLU function is given below:

                $$ReLU(x) = max(0, x)$$</dt>
            <dt>The ReLU function has the following properties:</dt>
            <dd>It is a simple, non-linear function.</dd>
            <dd>It is computationally efficient to compute.</dd>
            <dd>The output is sparse, which can help with overfitting.</dd>
        </dl>

        <img src="./images/ReLU-plot.png" alt="">
    </section>
    <section id="TanhFunction"><h3>Tanh Function</h3>
        <dl class="fa">
            <dt>The Tanh function is another popular activation function that maps any input value to a value between -1 and 1. The Tanh function is useful in cases where we want to introduce non-linearity and can help with normalization of the data. The mathematical formula for the Tanh function is given below:

                $$tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$
            </dt>
            <dt>The Tanh function has the following properties:</dt>
            <dd>It is a smooth, continuous function.</dd>
            <dd>Its output is always between -1 and 1.</dd>
            <dd>The output is centered around 0, which can help with normalization of the data.</dd>
        </dl>
        <img src="./images/Tanh-plot.png" alt="">
    </section>
    <!-- <section><h3>Softmax Function</h3>
        <dl class="fa">
            <dt>
                The Softmax function is commonly used in the output layer of Neural Networks that are designed for multi-class classification tasks. The Softmax function maps a vector of real-valued numbers to a probability distribution over the classes. The mathematical formula for the Softmax function is given below:

                $$softmax(x\_i) = \frac{e^{x\_i}}{\\sum\_{j=1}^{K} e^{x\_j}}$$

                where $K$ is the number of classes.
            </dt>
            <dt>
                The Softmax function has the following properties:</dt>
                <dd>It produces a probability distribution over the classes.</dd>
                <dd>The output values are always between 0 and 1 and sum up to 1.</dd>
        </dl>
        <img src="./images/softmax-plot.png" alt="softmax">
    </section> -->
</section>

<section id="ANNTraining" data-min="50" class="main-section-title"><h1>ANN training</h1></section>
<section id="BackpropagationExplained" class="sub-sections"><h2>ANN training. Backpropagation.</h2>
    <section id="TrainingProcess">
        <dl class="fa">
            <dt>Training an ANN involves adjusting the weights of the connections between neurons to minimize the error between the predicted output and the actual output. </dt>
            <dt>This is done using a process called backpropagation, which involves calculating the gradient of the error with respect to each weight and bias in the network, and using this gradient to update the values of the weights and biases in the direction that reduces the error.</dt>
        </dl>
        <a href="./images/BackpropagationSimple.gif"><img src="./images/BackpropagationSimple.gif" alt="Backpropagation"></a>
        <p style="font-size:0.6em">Backpropagation illustration by <a href="https://machinelearningknowledge.ai">machinelearningknowledge.ai</a></p>
    </section>
    <section id="BackpropagationAlgorithm"><h3>Backpropagation Algorithm</h3>
        <p>Here's how it works in simple terms:</p>
        <dl class="fa">
            <dt>1. Feedforward: The input data is fed into the network, and the output is calculated by passing it through a series of interconnected nodes, also known as neurons.</dt>
            <dt>2. Calculate Error: The difference between the predicted output and the actual output is calculated, and this is known as the error.</dt>
            <dt>3. Backpropagation: The error is then propagated backwards through the network to adjust the weights and biases. The weights and biases are updated in the direction that reduces the error, using an optimization algorithm such as Stochastic Gradient Descent.</dt>
            <dt>4. Repeat: Steps 1-3 are repeated for each example in the training dataset until the error is minimized.</dt>
        </dl>
    </section>
    <section id="BackpropagationReference"><h3>Reference</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>"Backpropagation, intuitively" - a video from <a href="https://www.youtube.com/@3blue1brown">3blue1brown</a> </dt>
            <iframe width="1582" height="597" src="https://www.youtube.com/embed/Ilg3gGewQ5U" title="Backpropagation, intuitively | Deep Learning Chapter 3" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </dl>
    </section>
</section>

<section id="ANNTypes" data-min="50" class="main-section-title"><h1>ANN Types</h1></section>
<section id="CommonANNArchitectures" class="sub-sections"><h2>ANN Types</h2>
    <section id="FNNs"><h3>Feedforward Neural Networks (FNNs)</h3>
        <a href="./images/Feedforward_DigitClassification.gif"><img src="./images/Feedforward_DigitClassification.gif" alt="Feedforward Neural Network" style="width: 60vw"></a>
        <dl class="fa">
            <dt>Feedforward Neural Networks (FNNs) are the simplest type of neural network, where the data flows only in one direction, from the input layer to the output layer. </dt>
            <dt>FNNs can have one or more hidden layers. </dt>
            <dt>The neurons in the input layer are connected to the neurons in the hidden layer, and the neurons in the hidden layer are connected to the neurons in the output layer.</dt>
            <dt>The primary use case for FFNs is to act as a universal function approximator for supervised learning tasks like classification and regression, mapping fixed-size inputs to fixed-size outputs by learning complex, non-linear relationships.</dt>
        </dl>
    </section>
   <section id="CNNs"><h3>Convolutional Neural Networks (CNNs)</h3>
       <a href="./images/Convolutional-Neural-Network.webp"><img src="./images/Convolutional-Neural-Network.webp" alt="Convolutional Neural Network Architecture"></a>

       <dl class="fa">
            <dt>CNNs are specialized deep learning models, primarily designed to process data that has a known grid-like topology, such as <b>images</b>.</dt>
            <dt>Instead of full connectivity, CNNs use <b>Convolutional Layers</b> where neurons connect only to a small, localized region of the preceding layer's input.</dt>
            <dt>This local connectivity allows the network to automatically learn and extract hierarchical spatial features, starting with simple patterns like edges and corners in the first layers.</dt>
            <dt>They use a combination of Convolutional, <b>Pooling</b> (for downsampling), and <b>Fully Connected (Dense)</b> layers to perform tasks like image classification, object detection, and image segmentation.</dt>
        </dl>
    </section>
    <section id="CNNReference"><h3>Reference</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The moment we stopped understanding AI [AlexNet]</dt>
            <iframe width="1218" height="685" src="https://www.youtube.com/embed/UZDiGooFs54" title="The moment we stopped understanding AI [AlexNet]" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </dl>
    </section>
    <section id="RNNs"><h3>Recurrent Neural Networks (RNNs)</h3>
        <a href="./images/RNN_state.gif"><img src="./images/RNN_state.gif" alt="Recurrent Neural Network Architecture"></a>
        <dl class="fa">
            <dt>Recurrent Neural Networks (RNNs) are designed to process <b>sequential data</b> like time series, text, or speech, where the order of information is crucial.</dt>
            <dt>Unlike FFNs, which operate on fixed input sizes and do not maintain any memory of previous inputs, RNNs can handle variable-length sequences of inputs while retaining information about the past</dt>
            <dt>RNNs have a <b>feedback loop</b> that allows the output of a neuron to be fed back into the input of the same neuron or other neurons in the network. </dt>
            <dt>This allows RNNs to model the temporal dependencies in sequential data.</dt>
        </dl>
    </section>
    <section id="LSTMs"><h3>Long Short-Term Memory Networks (LSTMs)</h3>
        <a href="./images/LSTM.png"><img src="./images/LSTM.png" alt="LSTM"></a>
        <dl class="fa">
            <dt>LSTMs are an advanced type of Recurrent Neural Network designed to overcome the <b>vanishing gradient problem</b> of standard RNNs.</dt>
            <dt>They achieve this by using a specialized internal structure called the <b>LSTM Cell/Unit</b>, which contains three key 'gates' that regulate the flow of information.</dt>
            <dt>The <b>Forget Gate</b> determines what information from the previous cell state should be discarded (forgotten).</dt>
            <dt>The <b>Input Gate</b> and <b>Output Gate</b> control which new information is added to the cell state and which part of the cell state is outputted as the hidden state.</dt>
            <dt>This regulated flow allows LSTMs to effectively capture and remember **long-term dependencies** in sequences, making them ideal for complex tasks like machine translation and speech recognition.</dt>
        </dl>
    </section>
    <section id="GANs"><h3>Generative Adversarial Networks (GANs)</h3>
       <a href="./images/GAN.png"><img src="./images/GAN.png" alt="Generative Adversarial Network Architecture"></a>
       <dl class="fa">
           <dt>GANs are a class of deep learning models used for <b>generative tasks</b>, where the goal is to generate new data samples that resemble a given training dataset.</dt>
           <dt>They consist of two main components: a <b>Generator</b> and a <b>Discriminator</b>.</dt>
           <dt>The Generator creates fake data samples, while the Discriminator evaluates them against real samples, providing feedback to the Generator.</dt>
           <dt>This adversarial process encourages the Generator to produce increasingly realistic samples over time.</dt>
       </dl>
    </section>
</section>
<section id="TransformerRevolution"><h2>The Transformer Architecture</h2>
    <section id="TransformerOverview"><h3>Overview</h3>
        <dl class="fa">
            <dt>The Transformer architecture was introduced in the paper <a href="https://arxiv.org/html/1706.03762v7">"Attention is All You Need"</a> by Vaswani et al. in 2017.</dt>
            <dt>It has since become the foundation for many state-of-the-art NLP models including BERT, GPT, and others.</dt>
            <dt>The Transformer is a deep neural network architecture that entirely replaced Recurrent Neural Networks (RNNs) for processing sequences like language.</dt>
            <dt><b>Parallel Processing</b>: The biggest advantage is that it can process an entire input sequence (like a sentence) all at once and in parallel, unlike RNNs, which had to process one word after the next. This makes training much faster.</dt>
            <dt><b>Self-Attention</b>: It introduced the revolutionary Self-Attention mechanism. This allows the model to instantly weigh the importance of every other word in the sequence when processing a specific word. This gives the model powerful context</dt>
        </dl>
    </section>
    <section id="TransformerComponents"><h3>Core Components</h3>
        <div style="display: flex; align-items: center">
            <div>
                <a href="./images/transformers.webp"><img src="./images/transformers.webp" alt="transformers.webp" style="width: 80vw;height: 90vh;"></a>
            </div>
            <dl class="fa" style="margin-left: 2em;">
                <dt><b>Encoder:</b> The part of the model that processes the input sequence and generates a context-aware representation.</dt>
                <dt><b>Decoder:</b> The part of the model that generates the output sequence from the encoded representation.</dt>
                <dt><b>Self-Attention Mechanism:</b> Allows the model to weigh the importance of different words in the input sequence when generating each word in the output sequence.</dt>
            </dl>
        </div>
    </section>
    <section id="TransformerReference"><h3>Reference</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Transformers, the tech behind LLMs @3Blue1Brown</dt>
            <iframe width="1582" height="597" src="https://www.youtube.com/embed/wjZofJX0v4M" title="Transformers, the tech behind LLMs | Deep Learning Chapter 5" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </dl>
    </section>

</section>

<section class="main-section-title" id="ProsAndConsOfANN"><h1>Pros and Cons of Artificial Neural Networks</h1></section>
<section class="sub-sections"><h2>Pros and Cons of Artificial Neural Networks</h2>
    <section id="ProsOfANN"><h3>Pros of Artificial Neural Networks</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>1. <strong>Ability to Learn Nonlinear Relationships:</strong></dt>
            <dd>ANNs can model complex nonlinear relationships between inputs and outputs, making them effective for tasks such as image recognition, speech processing, and more.</dd>

            <dt>2. <strong>Adaptability:</strong></dt>
            <dd>ANNs can be trained to solve a variety of problems without needing specific rules, allowing them to generalize to new data once trained.</dd>

            <dt>3. <strong>Scalability:</strong></dt>
            <dd>ANNs can scale from simple networks to deep architectures, allowing them to be used in small to large-scale applications (e.g., simple classification to deep learning tasks like NLP).</dd>
        </dl>
    </section>
    <section id="ConsOfANN"><h3>Cons of Artificial Neural Networks</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>1. <strong>Computationally Intensive:</strong></dt>
            <dd>Training ANNs, especially deep networks, requires significant computational resources, often involving GPUs or specialized hardware.</dd>

            <dt>2. <strong>Data Dependency:</strong></dt>
            <dd>ANNs require large datasets to achieve good performance. Without sufficient data, they are prone to overfitting or underfitting.</dd>

            <dt>3. <strong>Black Box Nature:</strong></dt>
            <dd>It is difficult to interpret the inner workings of ANNs, making it hard to explain why certain decisions are made, which can be a drawback in critical applications like healthcare or finance.</dd>
        </dl>
    </section>
</section>


<section class="main-section-title" id="ApplicationsOfANN"><h1>Applications of Artificial Neural Networks</h1></section>
<section class="sub-sections"><h2>Applications of Artificial Neural Networks</h2>
    <section id="LearningTasksForANN"><h3>Learning Tasks for Artificial Neural Networks</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Artificial Neural Networks (ANNs) are applied in various types of learning tasks:</dt>

            <dt>Supervised Learning</dt>
            <dd>ANNs are widely used for tasks like classification, regression, and predictive modeling, where labeled data is available (e.g., image recognition, speech recognition).</dd>

            <dt>Unsupervised Learning</dt>
            <dd>ANNs are employed in tasks like clustering, dimensionality reduction, and anomaly detection, typically using techniques like Autoencoders and Self-Organizing Maps (SOM).</dd>

            <dt>Reinforcement Learning</dt>
            <dd>ANNs are integrated into reinforcement learning for decision-making tasks, particularly in environments where agents learn from rewards (e.g., Deep Q-Networks, AlphaGo).</dd>
        </dl>
    </section>
    <section id="ImageRecognition"><h3>Image Recognition</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Convolutional Neural Networks (CNNs), a type of ANN, are widely used for image classification and object detection tasks.</dt>
            <dd>They have shown remarkable success in recognizing objects in photos, medical images, and facial recognition systems.</dd>
            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                # Example of loading a pre-trained CNN for image recognition
                from tensorflow.keras.applications import VGG16

                model = VGG16(weights='imagenet')
                # Model can now be used for image classification
            </code></pre>
        </dl>
    </section>

    <section id="NaturalLanguageProcessing"><h3>Natural Language Processing</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Recurrent Neural Networks (RNNs) and Transformer models are widely used in NLP tasks.</dt>
            <dd>Applications include language translation, sentiment analysis, and chatbots like GPT, which are based on Transformer architecture.</dd>
            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                # Example of using GPT-3 for text generation
                import openai

                openai.api_key = 'your-api-key'
                response = openai.Completion.create(
                    model="text-davinci-003",
                    prompt="Explain ANN in simple terms.",
                    max_tokens=50
                )
                print(response.choices[0].text.strip())
            </code></pre>
        </dl>
    </section>

    <section id="GameAI"><h3>Game AI</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>ANNs are used to train intelligent agents in games to make decisions, strategize, and even learn from the game environment.</dt>
            <dt>Deep Reinforcement Learning (which uses Deep Neural Networks) has been applied to create AI that can outperform humans in games like Go and Dota 2.</dt>
        </dl>
    </section>
</section>

<section class="main-section-title" id="UsingPythonLibrariesForANN"><h1>Using Python Libraries for Artificial Neural Networks</h1></section>
<section class="sub-sections"><h2>Using Python Libraries for Artificial Neural Networks</h2>
    <section id="TensorFlow"><h3>TensorFlow</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>TensorFlow is an open-source library developed by Google, widely used for building and training machine learning models, especially deep learning models.</dt>
            <dt>TensorFlow is highly scalable and can be deployed in both research and production environments. It supports both low-level operations and high-level APIs for neural networks.</dt>
        </dl>
    </section>
    <section id="Keras"><h3>Keras</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Keras is a high-level API built on top of TensorFlow that simplifies the process of creating and training deep learning models.</dt>
            <dt>Keras is known for its user-friendliness, ease of prototyping, and flexibility in building both simple and complex models.</dt>
        </dl>
    </section>
    <section id="KerasVsTensorFlow"><h3>Keras vs TensorFlow</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Purpose</dt>
            <dd>Keras: Simplifies model building with a high-level API.</dd>
            <dd>TensorFlow: Offers both high-level APIs and low-level control for custom models.</dd>

            <dt>Use Case</dt>
            <dd>Keras: Ideal for beginners and rapid prototyping.</dd>
            <dd>TensorFlow: Suitable for research, production, and complex projects.</dd>

            <dt>Flexibility</dt>
            <dd>Keras: Limited to predefined layers and operations.</dd>
            <dd>TensorFlow: Supports custom layers, operations, and optimizations.</dd>

            <dt>Deployment</dt>
            <dd>Keras: Easier to use in small-scale environments.</dd>
            <dd>TensorFlow: Supports large-scale deployments and integrations.</dd>
        </dl>
    </section>

    <section id="PyTorch"><h3>PyTorch</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>PyTorch is an open-source machine learning library developed by Facebook, widely used for research and prototyping.</dt>
            <dt>PyTorch provides dynamic computational graphs, allowing for easier debugging and experimentation by enabling code execution in smaller portions without needing to run the entire code.</dt>
            <dt>It is also known for its flexibility and ease of use in creating complex neural network architectures.</dt>
        </dl>
    </section>

</section>


<section class="main-section-title" id="ExampleUsingPyTorch"><h1>Example: Creating an ANN with PyTorch</h1></section>
<section class="sub-sections"><h2>Example: Creating an ANN with PyTorch</h2>
    <section id="PyTorchExample"><h3>PyTorch Example</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The network consists of three layers:</dt>
            <dd>1. <strong>Input Layer:</strong> An input layer with 8 features, passed directly into the first hidden layer.</dd>
            <dd>2. <strong>Hidden Layer:</strong> A fully connected (Dense) hidden layer with 32 neurons and ReLU activation, which processes the input data.</dd>
            <dd>3. <strong>Output Layer:</strong> A fully connected output layer with 1 neuron and a sigmoid activation function, producing a value between 0 and 1, suitable for binary classification tasks.</dd>

            <pre><code rel="Python" class="python" style="min-height: 70vh;">
                # Import necessary libraries
                import torch
                import torch.nn as nn
                import torch.optim as optim
                from torch.utils.data import TensorDataset, DataLoader # Import for batching
                from sklearn.model_selection import train_test_split
                from sklearn.datasets import make_classification

                # --- 1. DATA PREPARATION ---

                # Example dataset creation (1000 samples, 8 features, 2 classes)
                X, y = make_classification(n_samples=1000, n_features=8, n_classes=2, random_state=42)
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=0.2, random_state=42
                )

                # Convert data to PyTorch tensors
                X_train = torch.tensor(X_train, dtype=torch.float32)
                # Ensure y_train is float and correct shape for BCELoss
                y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
                X_test = torch.tensor(X_test, dtype=torch.float32)
                y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

                # --- 2. IMPLEMENT BATCHING ---

                BATCH_SIZE = 64 # Define a reasonable batch size
                NUM_EPOCHS = 100 # Increase epochs to allow convergence

                # Create TensorDatasets
                train_data = TensorDataset(X_train, y_train)
                test_data = TensorDataset(X_test, y_test)

                # Create DataLoaders for efficient iteration
                train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)
                test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)


                # --- 3. MODEL DEFINITION ---

                class SimpleANN(nn.Module):
                    def __init__(self):
                        super(SimpleANN, self).__init__()
                        self.hidden = nn.Linear(8, 32)
                        self.output = nn.Linear(32, 1)

                    def forward(self, x):
                        # ReLU activation for hidden layer (non-linearity)
                        x = torch.relu(self.hidden(x))
                        # Sigmoid activation for binary classification output
                        x = torch.sigmoid(self.output(x))
                        return x


                # --- 4. INITIALIZATION ---

                model = SimpleANN()
                criterion = nn.BCELoss()  # Binary Cross-Entropy loss for sigmoid output
                optimizer = optim.Adam(model.parameters(), lr=0.001)


                # --- 5. TRAIN THE MODEL (With Batching) ---

                print(f"Starting training for {NUM_EPOCHS} epochs...")
                for epoch in range(NUM_EPOCHS):
                    model.train() # Set model to training mode
                    current_loss = 0.0

                    # Iterate over the data in batches
                    for inputs, targets in train_loader:
                        # Zero the parameter gradients
                        optimizer.zero_grad()

                        # Forward + Backward + Optimize
                        outputs = model(inputs)
                        loss = criterion(outputs, targets)
                        loss.backward() # Backpropagation
                        optimizer.step() # Update weights

                        current_loss += loss.item() * inputs.size(0)

                    epoch_loss = current_loss / len(X_train)

                    # Print less often for longer training
                    if (epoch + 1) % 10 == 0:
                        print(f"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {epoch_loss:.4f}")


                # --- 6. EVALUATE THE MODEL ---

                correct = 0
                total = 0
                model.eval() # Set model to evaluation mode

                with torch.no_grad():
                    for inputs, targets in test_loader:
                        test_outputs = model(inputs)
                        # Convert sigmoid output (0 to 1) to binary prediction (0 or 1)
                        predicted = (test_outputs >= 0.5).float()

                        total += targets.size(0)
                        correct += (predicted.eq(targets).sum().item())

                accuracy = correct / total

                print("\n--- Final Results ---")
                print(f"Total training samples: {len(X_train)}")
                print(f"Total test samples: {len(X_test)}")
                print(f"Test Accuracy: {accuracy * 100:.2f}%")
            </code></pre>
        </dl>
    </section>
</section>


<section id="HandsOnImageClassification" class="main-section-title"><h1>Hands on: Image Classification with ANN</h1></section>
<section id="PracticeImageClassification" class="sub-sections"><h2>Hands on: Image Classification with ANN</h2>
    <section id="HandsOnImageClassificationCode">
        <dl class="fa" style="min-width:80vw">
            <dt>We will perform Image Classification using a pre-trained CNN model</dt>
            <dd><a href="https://docs.pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html">VGG16 Model Documentation</a></dd>
            <dt>Steps:</dt>
            <dd>1. Load the pre-trained VGG16 model.</dd>
            <dd>2. Preprocess the input images to match the model's expected input format.</dd>
            <dd>3. Use the model to predict the class of each image.</dd>
            <dt>Code Example:</dt>
            <pre><code rel="Python" class="python" style="min-height: 40vh;">
                import torch
                import torch.nn as nn
                import requests
                from PIL import Image
                from torchvision import models, transforms
                import os

                # --- Configuration ---
                MODEL_DIR = "model"
                WEIGHTS_FILENAME = "vgg16_weights.pth"
                LOCAL_PATH = os.path.join(MODEL_DIR, WEIGHTS_FILENAME)
                IMAGE_DIR = "images"  # New directory variable


                IMAGE_FILENAMES = (
                    "cat1.webp",
                    "lynx.jpg",
                    "elephant.jpg",
                )


                def setup_model_and_weights():
                    """Initializes VGG16 and loads weights from a local path, downloading if necessary."""
                    print("Setting up VGG16 model and loading weights...")
                    os.makedirs(MODEL_DIR, exist_ok=True)

                    if not os.path.exists(LOCAL_PATH):
                        print(f"Weights not found locally at {LOCAL_PATH}. Downloading now...")
                        VGG16_DOWNLOAD_URL = models.VGG16_Weights.DEFAULT.url

                        # Download and save the state dictionary
                        state_dict = torch.hub.load_state_dict_from_url(
                            VGG16_DOWNLOAD_URL, model_dir=MODEL_DIR, file_name=WEIGHTS_FILENAME
                        )
                        print(f"Weights successfully downloaded and saved to {LOCAL_PATH}.")
                    else:
                        print(f"Weights found locally at {LOCAL_PATH}. Loading from disk...")
                        state_dict = torch.load(LOCAL_PATH)

                    model = models.vgg16(weights=None)
                    model.load_state_dict(state_dict)
                    model.eval()

                    return model


                def get_imagenet_class_names():
                    """Downloads the ImageNet class index file and returns a list of names."""
                    LABELS_URL = (
                        "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
                    )

                    try:
                        response = requests.get(LABELS_URL)
                        class_names = [
                            line.strip() for line in response.text.split("\n") if line.strip()
                        ]
                        if len(class_names) == 1000:
                            return class_names
                        else:
                            print("Warning: Could not load exactly 1000 class names.")
                            return None
                    except Exception as e:
                        print(f"Error loading class names: {e}")
                        return None


                def prepare_image(filename):
                    """Loads an image, applies VGG16 preprocessing, and returns the batch tensor."""

                    file_path = os.path.join(IMAGE_DIR, filename)

                    # PyTorch models require specific transformations (resize, center crop, normalize)
                    preprocess = transforms.Compose(
                        [
                            transforms.Resize(256),
                            transforms.CenterCrop(224),
                            transforms.ToTensor(),
                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                        ]
                    )

                    try:
                        if not os.path.exists(file_path):
                            raise FileNotFoundError(f"File not found at: {file_path}")

                        # Open the image directly from the local file path ðŸš¨
                        img = Image.open(file_path).convert(
                            "RGB"
                        )  # Use .convert('RGB') to handle WEBP/PNG transparency issues
                        print(f"Image loaded from disk: {filename}. Size: {img.size}")

                        # Apply transforms
                        img_t = preprocess(img)

                        # Add a batch dimension: (C, H, W) -> (1, C, H, W)
                        img_batch = img_t.unsqueeze(0)

                        return img_batch

                    except Exception as e:
                        print(f"ERROR: Could not process image file '{filename}': {e}")
                        return None


                def run_inference(model, input_batch):
                    """Makes the prediction and returns the raw output."""
                    print("\nRunning classification...")
                    with torch.no_grad():
                        output = model(input_batch)
                    return output


                def display_results(output, class_labels, top_k=5):
                    """Converts model output to probabilities, finds top K, and prints human-readable labels."""
                    probabilities = nn.Softmax(dim=1)(output)[0]
                    top_prob, top_catid = torch.topk(probabilities, top_k)

                    print(f"\n--- Top {top_k} Predictions ---")
                    for i in range(top_prob.size(0)):
                        index = top_catid[i].item()
                        confidence = top_prob[i].item()

                        if index < len(class_labels):
                            label = class_labels[index].capitalize()
                        else:
                            label = f"Index {index} (Label List Incomplete)"

                        print(f"{i + 1}. {label} (Confidence: {confidence:.2f})")
                    print("--------------------------")


                def main():
                    model = setup_model_and_weights()
                    class_labels = get_imagenet_class_names()
                    if class_labels is None:
                        return

                    print("\nStarting local image classification batch...\n")

                    # Loop through all specified filenames
                    for i, filename in enumerate(IMAGE_FILENAMES):
                        print("\n=======================================================")
                        print(f"Processing Image {i + 1} of {len(IMAGE_FILENAMES)}: {filename}")
                        print("=======================================================")

                        # Prepare Image (now loads from disk)
                        input_batch = prepare_image(filename)

                        if input_batch is not None:
                            output = run_inference(model, input_batch)
                            display_results(output, class_labels, top_k=5)
                        else:
                            # Prints the error message from within prepare_image
                            continue


                if __name__ == "__main__":
                    main()

            </code></pre>

        </dl>
    </section>
</section>


<section id="IntroductionToLLM" class="main-section-title"><h1>Introduction to Large Language Models (LLMs)</h1></section>
<section id="LLMOverview" class="sub-sections"><h2>Introduction to LLMs</h2>
    <section id="WhatIsLLM"><h3>What is an LLM?</h3>
        <dl class="fa">
            <dt>Large Language Models (LLMs) are deep learning models trained on humongous datasets (internet data, books, etc.) to understand, generate, and manipulate human language.</dt>
            <dt>They are built on the <b>Transformer</b> architecture, scaling up the number of parameters and training data.</dt>
            <dt>LLMs are "Foundation Models" - trained once on vast data, then adapted (fine-tuned) for specific tasks.</dt>
            <dt><i>The rule of thumb</i>: a model is an LLM if it is a Transformer-based language model capable of broad, general-purpose reasoning  generally requiring at least 7 billion parameters.</dt>

        </dl>
    </section>
    <section><h3>How LLMs Work?</h3>
        <dl class="fa">
            <dt>During inference, LLMs generate text by sampling from the learned probability distribution of tokens, conditioned on the input prompt.</dt>
        </dl>
        <a href="./images/LLM_Next_Word_Prediction.png"><img src="./images/LLM_Next_Word_Prediction.png" alt="LLM_Next_Word_Prediction.png"></a>
    </section>
    <section><h3>Reference</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Large Language Models explained briefly @3Blue1Brown</dt>
            <iframe src="https://www.youtube.com/embed/LPZh9BOjkQs" title="Large Language Models explained briefly" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen style="height: 70vh; margin-top: 1em;"></iframe>
        </dl>
    </section>
    <section id="LLMKeyConcepts"><h3>Key Concepts</h3>
        <dl class="fa">
            <dt><b>Tokens</b>: Text is broken down into chunks called tokens (words, subwords, or characters). Models process tokens, not raw text.</dt>
            <dd>1 token $\approx$ 0.75 word.</dd>
            <dd>1000 tokens $\approx$ 750 words.</dd>
            <dd>1M tokens $\approx$ 750,000 words, 5 to 10 average-length novels (a 300-page book is often around 150,000 to 200,000 tokens).</dd>
            <dt>Reference: <a href="https://platform.openai.com/tokenizer">OpenAI Tokenizer</a></dt>
            <dt><b>Context Window</b>: The maximum number of tokens the model can consider at one time (input + output).</dt>
            <dt><b>Embeddings</b>: Tokens are converted into high-dimensional vectors capturing semantic meaning.</dt>
            <dt><b>Parameters</b>: The internal variables (weights) learned during training. More parameters generally mean improved capabilities (e.g., Llama-3-8B vs Llama-3-70B).</dt>
        </dl>
    </section>
    <section><h3>LLM Scaling Laws</h3>
        <dl class="fa">
            <dt>LLM performance improves predictably with increased model size, dataset size, and compute resources.</dt>
            <dt>Doubling the number of parameters typically leads to a significant reduction in error rates across various NLP tasks.</dt>
            <dt>However, returns diminish at extreme scales, and practical considerations (cost, latency) become critical.</dt>
        </dl>
        <a href="./images/LLM_ScalingLaws.png"><img src="./images/LLM_ScalingLaws.png" alt="LLM_ScalingLaws" style="height: 80vh;"></a>
        <p style="font-size:0.6em">Image from <a href="https://towardsdatascience.com/scaling-law-of-language-models-5759de7f830c/">Scaling Law Of Language Models</a> @towardsdatascience.com</p>
    </section>
    <section id="InferenceParameters"><h3>Controlling the output generation</h3>
        <dl class="fa">
            <dt><b>Max Tokens</b>: Limits the length of the generated output.</dt>
            <dt><b>Temperature</b>: Controls randomness. Low (0.1) = deterministic/focused; High (0.8+) = creative/random.</dt>
            <dt><b>Top-k</b>:  the model only considers the $k$ most likely next tokens at each step, instead of all possible tokens.</dt>
            <dd>Effect: a smaller $k$ (e.g., $k$=5) makes the output highly predictable, safe, and focused, as it almost always chooses one of the few most probable words. A larger $k$ (e.g., $k$=100) increases diversity and creativity.</dd>
            <dt><b>Top-p</b> (nucleus sampling): the model considers the smallest set of tokens whose cumulative probability sums up to the threshold $p$ (a value between 0.0 and 1.0).</dt>
            <dd>Effect: a smaller $p$ (e.g., $p$=0.5) makes the output highly predictable and focused, keeping only the most probable tokens. A larger $p$ (e.g., $p$=0.95) increases diversity by including more low-probability tokens, allowing for more creative and varied text.</dd>
            <dt>By adjusting these parameters, you can precisely control the balance between deterministic/coherent output and creative/diverse output.</dt>
            <dt>Reference: <a href="https://poloclub.github.io/transformer-explainer/">Transformer explainer</a></dt>
        </dl>
    </section>
    <section id="LLMLifecycle"><h3>LLM Training Lifecycle</h3>
        <dl class="fa">
            <dt>1. <b>Pre-training</b>: Computationally expensive phase learning language patterns from massive amount of data.</dt>
            <dt>2. <b>Supervised Fine-Tuning (SFT)</b>: Tuning the model on high-quality instruction-response pairs to follow instructions.</dt>
            <dt>3. <b>RLHF/RLAIF</b>: Reinforcement Learning from Human/AI Feedback to align the model with human preferences (e.g., safety, helpfulness).</dt>
        </dl>
    </section>
    <section id="LLMBusinessUseCases"><h3>Business Use Cases</h3>
        <dl class="fa">
            <dt><b>Retrieval Augmented Generation (RAG)</b>: Connecting LLMs to company data (docs, databases) to answer internal queries accurately without hallucinations.</dt>
            <dt><b>Customer Support Chatbots</b>: Automating Tier-1 support with intelligent agents that understand intent and context.</dt>
            <dt><b>Content Generation</b>: detailed marketing copy, emails, and reports.</dt>
            <dt><b>Sentiment Analysis</b>: Analyzing customer feedback at scale.</dt>
        </dl>
    </section>
    <section id="LLMLeaderboard"><h3>LLM Leaderboards</h3>
        <dl class="fa">
            <dt><a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/">Open LLM Leaderboard @huggingface.co</a> - one of the most cited resource for tracking the quantitative, benchmark-based performance of models whose weights are released to the public</dt>
            <dt><a href="https://lmarena.ai/leaderboard">lmarena.ai</a> - this leaderboard evaluates both open and proprietary models (including major commercial models) based on real-world, user-driven feedback.</dt>
            <dt><a href="https://artificialanalysis.ai/leaderboards/models">LLM Leaderboard - Comparison of over 100 AI models from OpenAI, Google, DeepSeek & others</a></dt>
        </dl>
    </section>
</section>


<section class="main-section-title"><h1>Hands on: text generation demo with GPT2</h1></section>
<section id="HandsOnTextGenerationDemoWithGpt2" class="sub-sections"><h2>Hands on: text generation demo with GPT2</h2>
    <section><h3>Overview</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>In next example we will implement text generation using a pre-trained language model (<a href="https://huggingface.co/distilbert/distilgpt2">DistilGPT-2 @Hugging Face</a>).</dt>
            <dt>The model and the code run on local machine which gives us full control and privacy.</dt>
            <dd>By default, the model is downloaded into <code>~/.cache/huggingface</code> for Linux/MacOS, <code>C:\Users\&lt;YourUsername&gt;\.cache\huggingface\</code> for Windows</dd>
            <dt>We will provide a starting phrase, and the model predicts what words should come next, similar to how autocomplete works on phone but more sophisticated.</dt>
            <dt>The Transformers library gives us easy access to the pre-trained DistilGPT-2 model and its tokenizer, allowing us to load and use the model with just a few lines of code instead of training one from scratch.</dt>
        </dl>
    </section>
    <section><h3>The code</h3>
        <dl class="fa" style="min-width:80vw">
            <pre><code rel="Python" class="python" style="min-height: 99vh;">
                import torch
                from transformers import AutoModelForCausalLM, AutoTokenizer

                # --- Configuration ---
                MODEL_NAME = "distilgpt2"  # A small, fast GPT-2 model


                # 1. Setup Model and Tokenizer
                def setup_lm():
                    """Initializes the tokenizer and the pre-trained language model."""
                    print(f"Setting up Language Model: {MODEL_NAME}...")

                    # The tokenizer handles converting raw text into the numerical IDs the model understands.
                    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

                    # The model is loaded with pre-trained weights for text generation (Causal LM).
                    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
                    model.eval()  # Set to evaluation mode

                    print("Model and Tokenizer loaded successfully.")
                    return model, tokenizer


                # 2. Prepare Input
                def prepare_text_input(tokenizer, prompt_text):
                    """Encodes the input text string into a PyTorch tensor of token IDs."""
                    print(f"\nEncoding Prompt: '{prompt_text}'")

                    # The tokenizer encodes the text. return_tensors='pt' ensures PyTorch tensor output.
                    input_ids = tokenizer.encode(prompt_text, return_tensors="pt")

                    return input_ids


                # 3. Run Inference
                def run_generation(model, tokenizer, input_ids, max_length=50):
                    """Generates new text based on the input tokens."""
                    print("Running text generation...")

                    # Generates text. max_length specifies how long the final output sequence should be.
                    with torch.no_grad():
                        output_ids = model.generate(
                            input_ids,
                            max_length=max_length,
                            do_sample=True,
                            top_k=50,
                            top_p=0.95,
                            pad_token_id=tokenizer.eos_token_id,
                        )
                    return output_ids


                # 4. Display Results
                def display_results(tokenizer, generated_output_ids, prompt_length):
                    """Decodes the output tensor into human-readable text."""

                    # Decode the output tensor back into a string.
                    full_text = tokenizer.decode(generated_output_ids[0], skip_special_tokens=True)

                    # Separate the original prompt from the generated text
                    original_prompt = full_text[:prompt_length]
                    generated_text = full_text[prompt_length:].strip()

                    print("\n--- Generation Results ---")
                    print(f"Original Prompt: {original_prompt}")
                    print("-" * 30, "\n")
                    print(f"Generated Text: {generated_text}")
                    print("-" * 30, "\n")


                def main():
                    model, tokenizer = setup_lm()

                    # Text Input
                    PROMPT = "The purpose of life is"

                    input_ids = prepare_text_input(tokenizer, PROMPT)

                    # Get the length of the prompt in tokens for cleaner display
                    prompt_length = len(tokenizer.decode(input_ids[0], skip_special_tokens=True))

                    output_ids = run_generation(model, tokenizer, input_ids, max_length=50)

                    display_results(tokenizer, output_ids, prompt_length)


                if __name__ == "__main__":
                    main()

            </code></pre>
        </dl>
    </section>
</section>

<section class="main-sesction-title" id="introduction-to-gen-ai-api"><h1>Introduction to LLM Frontier Models API</h1></section>
<section class="sub-sections"><h2>How to use APIs on frontier LMM models?</h2>

    <section id="what-is-an-api"><h3>What is an API? (For Beginners)</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The "Waiter" Analogy</dt>
            <dd>
                Imagine you are at a restaurant. You (the user) are sitting at a table, and the kitchen (Google's servers) is in the back. You cannot walk into the kitchen and cook the food yourself.
                <br><br>
                You need a messenger. This messenger is the <strong>waiter</strong> (the API).
                <ul>
                    <li>You give the waiter your order (the <strong>API Request</strong>).</li>
                    <li>The waiter takes it to the kitchen.</li>
                    <li>The kitchen prepares the food (generates the text).</li>
                    <li>The waiter brings the food back to you (the <strong>API Response</strong>).</li>
                </ul>
            </dd>
            <dt>Why use the Google API?</dt>
            <dd>Using the API allows you to integrate Google's Gemini models directly into your code. Unlike the web interface, the API is programmable, allowing you to build automated tools, chatbots, or data analyzers.</dd>
        </dl>
    </section>

    <section id="api-keys-and-authentication"><h3>API Keys and Authentication</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Obtaining an API Key</dt>
            <dd>To access the kitchen, you need a pass. Google manages this through <strong>Google AI Studio</strong>. You can generate an API key there for free. This key identifies your project and enforces rate limits.</dd>
            <dt><b><span class="note">Security Warning</span></b></dt>
            <dd>Your API key is a <strong>secret</strong>! Never commit it to GitHub or share it publicly. If someone steals it, they can use your quota.</dd>
        </dl>
    </section>
    <section><h3>Using Google SDK with Python</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>To use Google genai models in our Python programs, first install the required packages:</dt>
            <pre><code rel="Terminal" class="shell">
                pip install google-genai python-dotenv
            </code></pre>
            <dt>Create a <code>.env</code> file in your project directory to store your API key securely:</dt>
            <pre><code rel=".env" class="text">
                GOOGLE_API_KEY=AIzaSy...your_actual_key_here
            </code></pre>
            <dt>The <code>google-genai</code> package provides the Python SDK for Google's Generative AI models, while <code>python-dotenv</code> helps load environment variables from the .env file, keeping your API key secure and out of your code.</dt>
            <dt>Example code:</dt>
            <pre><code rel="Python" class="python" style="min-height: 10vh;">
                import os
                from google import genai
                from dotenv import load_dotenv

                ### AUTHENTICATION
                # Load environment variables from a .env file located in the same directory.
                # Your .env file should contain: GOOGLE_API_KEY="AIzaSy..."
                load_dotenv()

                my_api_key = os.getenv("GOOGLE_API_KEY")

                if not my_api_key:
                    print("Error: GOOGLE_API_KEY not found in .env")
                    exit()

                client = genai.Client(api_key=my_api_key)

                ### Query the model:
                MODEL_NAME = "gemini-2.5-flash"
                prompt = "Tell me a witty joke about Python programmers."

                print("Sending request to Google...")
                response = client.models.generate_content(model=MODEL_NAME, contents=prompt)

                # print response
                print("Response received:")
                print(response.text)
            </code></pre>
        </dl>
    </section>
    <section id="api-settings-and-parameters"><h3>Model Selection</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>Google offers different versions of Gemini models, like:
                <ul>
                    <li><strong>gemini-2.0-flash-lite</strong>: smallest and most cost effective model, built for at scale usage.</li>
                    <li><strong>gemini-3-pro-preview</strong>: the best model in the world for multimodal understanding, and Google's most powerful agentic and vibe-coding model yet.</li>
                </ul>
            </dt>
            <pre><code rel="Python" class="python" style="min-height: 1vh;">
                import os
                from google import genai
                from dotenv import load_dotenv

                # 1. Load the .env file
                load_dotenv()
                my_api_key = os.getenv("GOOGLE_API_KEY")

                # Check if key exists to prevent vague errors
                if not my_api_key:
                    print("Error: GOOGLE_API_KEY not found. Check your .env file.")
                    exit()

                # 2. Initialize the Client with the key
                client = genai.Client(api_key=my_api_key)

                print("List of models that support generateContent:\n")
                for m in client.models.list():
                    for action in m.supported_actions:
                        if action == "generateContent":
                            print(m.name)

            </code></pre>
        </dl>
    </section>

    <section id="pricing-and-usage-limits"><h3>Pricing and The Free Tier</h3>
        <dl class="fa" style="min-width:80vw">
            <dt>The "Free of Charge" Tier</dt>
            <dd>Google offers a genuine <strong>Free Tier</strong> (unlike a temporary trial).
                <ul>
                    <li><strong>Cost:</strong> $0.00 / month.</li>
                    <li><strong>Rate Limits:</strong> You are limited to a certain number of requests per minute (RPM) and requests per day (RPD). For Gemini 1.5 Flash, this is currently 15 RPM and 1,500 RPD (subject to change).</li>
                    <li><strong>Data Usage:</strong> In the free tier, Google may use your inputs and outputs to improve their products. Do not send sensitive personal data in the free tier.</li>
                </ul>
            </dd>
            <dt>Pay-as-you-go</dt>
            <dd>If you need higher rate limits or <strong>data privacy</strong> (where Google does not train on your data), you switch to the paid plan. Pricing is based on tokens (characters/words) processed.</dd>
            <dt>Handling Rate Limits</dt>
            <dd>If you send requests too fast in the Free Tier, you will get a <code>429 Resource Exhausted</code> error. Your code should handle this gracefully.</dd>
            <dt>Reference: <a href="https://ai.google.dev/gemini-api/docs/pricing">Gemini Developer API pricing

</a></dt>
        </dl>
    </section>
</section>


<section class="disclaimer" data-background="/ProgressBG-ChatGPT_and_ML-Slides/outfit/images/for_slides/the_end_on_sand.jpg">
     <p>These slides are based on</p>
     <p>customised version of </p>
     <p><a href="http://hakim.se/">Hakimel</a>'s <a href="http://lab.hakim.se/reveal-js">reveal.js</a></p>
     <p>framework</p>
</section>
<!--
########################################################
##################### SLIDES END   #####################
########################################################
-->
        </div>
    </div>
    <!-- Custom processing -->
    <script src="/ProgressBG-ChatGPT_and_ML-Slides/outfit/js/slides.js"></script>
    <!-- external scripts -->
    <script src="/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/lib/js/head.min.js"></script>
    <script src="/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/js/reveal.js"></script>
     <!-- init reveal -->
    <script>
        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        var highlightjsTabSize = '  ';
        Reveal.initialize({
            controls: true,
            progress: true,
            slideNumber: 'c/t',
            keyboard: true,
            history: true,
            center: true,
            width: 1920,
            height: 1280,
            // Bounds for smallest/largest possible scale to apply to content
            // minScale: .5,
            maxScale: 1,
            // slide transition
            transition: 'concave', // none/fade/slide/convex/concave/zoom
            // Factor of the display size that should remain empty around the content
            margin: 0.1,
            // shift+mouse click to zoom in/out element
            zoomKey: 'ctrl',
            // theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
            // transition: Reveal.getQueryHash().transition || 'default'
            // Optional reveal.js plugins
            dependencies: [
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure(); hljs.initHighlightingOnLoad(); } },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/zoom-js/zoom.js', async: true },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/notes/notes.js', async: true },
                { src: '/ProgressBG-ChatGPT_and_ML-Slides/lib/reveal.js/plugin/math/math.js', async: true }
            ]
        });
    </script>
</body>
</html>
